\documentclass[10pt]{article}
\usepackage[]{mcode}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{natbib}

\begin{document}
\section{Central Limit Theorem}
Consider a sample of 1000 observations from a Uniform or Gaussian population distribution. These distributions have been recorded and binned into histograms in figure 1 and 2 below. As expected the uniform distribution follows a roughly box shaped probability density function and the Gaussian distribution follows the well known 'Bell shaped curve'. 

%\\ \mcode{distHist(1000, 'uniform'); distHist(1000, 'gaussian');}
%\lstinputlisting{C:/Users/Alun/Documents/Work/DataScience/Southampton/Machine_learning/MachineLearning/Labone/distHist.m}

\begin{figure}[!h]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{uniform.jpg}
    \caption{Sampling Uniform Distribution}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{gaussian.jpg}
    \caption{Sampling Normal Distribution}
  \end{minipage}
\end{figure}

%\lstinputlisting{C:/Users/Alun/Documents/Work/DataScience/Southampton/Machine_learning/MachineLearning/Labone/clt.m}

The central limit theorem states that if we take a sample of a population distribution and apply a statistic to it, for instance calculate the mean or sum of the sample then the distribution of our statistic itself approximates a Normal distribution, centred on the mean of the population distribution and with variance of $s^2 = \frac{\sigma^2}{n}$. This principal is true regardless of the population distribution originally sampled, for example in figures 3,4,5 below a uniform distribution has been sampled to produce a statistic. 

The approximation limits to the Gaussian distribution for large samples of n, although different population distributions can limit at different rates. Shown below are histograms sampling the difference between Two 12 observation samples of a uniform distribution for different sample sizes. 

\begin{figure}[!h]
  \centering
  \begin{minipage}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{clt100.jpg}
    \caption{10 Samples}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{clt1000.jpg}
    \caption{1000 Samples}
  \end{minipage}
  \begin{minipage}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{clt100000.jpg}
    \caption{100000 Samples}
  \end{minipage}
\end{figure}

We can see the distribution approximates to a bell shaped curve for large values of n. Also The variance of the distribution decreases for larger values of n, with data typically sitting closer to the mean. 

\section{}
\subsection{Manipulating covariance}

Multi-dimensional normally distributed datasets can be produced with desired shapes as long as the desired covariance matrix $C$ is known. To produce these distributions first factorise $C$ such that $A'A = C$. Then transform our standard two dimensional normal distribution $Y = X*A$. This has been done for an example covariance matrix C = [2,1; 1,2] in figure 6 below. 

\begin{figure}[!h]
  \centering
  \begin{minipage}[b]{0.3\textwidth}
      \includegraphics[width=\textwidth]{twoDGaussian.jpg}
      \caption{Normal distribution (teal) \& manipulated distribution (pink)}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{varianceProjection.png}
     \caption{Diagram describing how dot product projects the variance}
  \end{minipage}
  \begin{minipage}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{variance.jpg}
     \caption{How projected variance varies with angle (degrees)}
  \end{minipage}
\end{figure}

Using the cov() function in matlab yields a covariance of C verifying our goal of producing a desired covariance.

\subsection{Dimensionality reduction}  
In dimensionality reduction the desire is to simplify the features of a dataset while retaining as much information as possible. This is done by projecting the n dimensional data onto a lower dimension plane, in this case a vector. In order to choose the vector to project onto, we want to minimise the information lost. This can be done by maximising the variance of our data along the vector (once projected each observation is described only by its position on the vector, so variance of this position is an analogue to data retention). Figure 7 shows the mechanism for this. $ X*u $

The vector $u = [sin(theta_), cos(theta)]$ for $\theta \in {0-2\pi}$ draws a circle around the origin. As the u vector rotates we can see how well fitted the different lines are to our data. Figure 8 plots this, we can see the projection which has maximum variance (closest to the natural projection of the data) is slightly below 50 degrees. 

Using matlab's $eig()$ function we can extract the eigenvectors of the covariance matrix. The first eigenvector is equal to the principle component, the second is orthogonal to this (highest variance orthogonal vector). \\ $^{[https://www.quora.com/What-is-an-eigenvector-of-a-covariance-matrix]}$

\bibliographystyle{biblo.bib}

\end{document}

