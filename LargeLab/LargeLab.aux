\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Neural Networks}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Two Class Classification}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $p=0.5$ classification boundaries of two multivariate Gaussian distributions sampled 200 times. Blue - Posterior probability. Magenta, Red, Green are single layer feed forward networks using Bayesian regularisation with 3,20 and 80 nodes respectively \relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:NNcontours}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Top Left: Posterior probability for distribution 1 and prediction for 3,20 and 80 node single layer feed forward neural network using Bayesian regularisation for top right, bottom left and bottom right respectively \relax }}{2}}
\newlabel{fig:NNsurf}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Chaotic Time Series}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Relationship between training set and testing set error for a single layer feed forward neural network classifying the distributions described in eq.(1,2) with Levenverg-Marquardt gradient descent \relax }}{2}}
\newlabel{fig:NNnodes}{{3}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Prediction heat maps of 3 different training algorithms: Levenverg-Marquardt(left), Bayesian Regularisation(centre) and BFGS Quasi-Newton gradient descent\relax }}{2}}
\newlabel{fig:NNalg}{{4}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Mackey-Glass time series over an interval of 200, First 150 observations given as training data (blue line), One step prediction (red) and free running prediction (yellow)\relax }}{3}}
\newlabel{fig:Mackey-Glass}{{5}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Financial Time Series}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces RMSE for 80 node 1 layer feed forward neural network and linear regression predicting the Mackey-Glass time series ($tau =17$); predicting on the training set, iterative one step prediction and Free running prediction \relax }}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Change in root mean square error for an 80 node single layer feed forward neural network to characterise the Mackey Glass equation for a $\tau $ of 17 as a function of time "remembered". \relax }}{4}}
\newlabel{fig:linMemory}{{6}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Prediction values of a 80 node neural network t modelling Mackey-Glass with a $\tau $ of 17 for both training set (purple), one step prediction(green) and free flowing linear algorithms (yellow) \relax }}{4}}
\newlabel{fig:nnMackeyGlass}{{7}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  FTSE closing prices for the last year \relax }}{4}}
\newlabel{fig:FTSEclose}{{8}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  FTSE internal training prediction of [30 30 30] nodes 3 layer network using Bayesian regularisation. Including volume of trades information \relax }}{5}}
\newlabel{fig:FTSEtrainVol}{{9}{5}}
